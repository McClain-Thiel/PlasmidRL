# BioNeMo container image
IMAGE=nvcr.io/nvidia/clara/bionemo-framework:2.6.3

# Host paths
PROJECT_HOST_DIR=/mcclain/projects/PlasmidRL
DATA_FASTA=/mcclain/projects/data/sequences.fasta
CACHE_DIR=/mcclain/.cache
TMP_DIR=/mcclain/tmp

# HuggingFace caches (kept on /mcclain)
HF_HOME=/mcclain/.cache/huggingface
HF_HUB_CACHE=/mcclain/.cache/huggingface/cache

# Python/pip/uv caches (kept on /mcclain)
PIP_CACHE_DIR=/mcclain/.cache/pip
UV_CACHE_DIR=/mcclain/.cache/uv

# Container working directory mapping
WORKDIR=/workspace

# Docker runtime options
SHM_SIZE=8g
DOCKER_GPU_FLAG="--gpus all"
DOCKER_IPC_FLAG=--ipc=host
DOCKER_ULIMIT_FLAGS="--ulimit memlock=-1 --ulimit stack=67108864"

# Model conversion settings
MODEL_PATH=hf://arcinstitute/savanna_evo2_1b_base
MODEL_SIZE=1b
OUTPUT_NEMO_DIR=/mcclain/models/nemo2_evo2_1b_8k

# Preprocessing config
PREPROC_JSON=sft/preprocess_evo2.json
PREPROC_OUTPUT_DIR=/mcclain/datasets/evo2_preprocessed/sequences
OUTPUT_PREFIX=sequences

# Training config
TRAIN_DATASET_CONFIG=sft/train_evo2.yaml
NUM_NODES=1
DEVICES=1
SEQ_LENGTH=2048
TP_SIZE=1
PP_SIZE=1
CP_SIZE=1

# Training knobs for memory
MODEL_SIZE=1b
MICRO_BATCH_SIZE=1
GRAD_ACC_BATCHES=16
ACT_CKPT_LAYERS=9999
FP8=false
WORKERS=4
CKPT_DIR=/mcclain/models/nemo2_evo2_1b_8k

# Weights & Biases
# Set your WANDB_API_KEY here or export it in your shell before running
WANDB_ENTITY=mcclain
WANDB_PROJECT=plasmidrl
WANDB_TAGS="evo2 plasmidrl sft"
WANDB_GROUP=
WANDB_JOB_TYPE=finetune
WANDB_RUN_NAME=

