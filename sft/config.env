# BioNeMo container image
IMAGE=nvcr.io/nvidia/clara/bionemo-framework:2.6.3

# Host paths
PROJECT_HOST_DIR=/efs/projects/PlasmidRL
DATA_FASTA=/efs/projects/data/sequences.fasta
CACHE_DIR=/efs/.cache
TMP_DIR=/tmp

# HuggingFace caches (kept on /mcclain)
HF_HOME=/efs/.cache/huggingface
HF_HUB_CACHE=/efs/.cache/huggingface/cache

# Python/pip/uv caches (kept on /mcclain)
PIP_CACHE_DIR=/efs/.cache/pip
UV_CACHE_DIR=/efs/.cache/uv

# Container working directory mapping
WORKDIR=/workspace

# Docker runtime options
SHM_SIZE=8g
DOCKER_GPU_FLAG="--gpus all"
DOCKER_IPC_FLAG=--ipc=host
DOCKER_ULIMIT_FLAGS="--ulimit memlock=-1 --ulimit stack=67108864"

# Model conversion settings
MODEL_PATH=hf://arcinstitute/savanna_evo2_1b_base
MODEL_SIZE=1b
OUTPUT_NEMO_DIR=/efs/models/nemo2_evo2_1b_8k

# Preprocessing config
PREPROC_JSON=sft/preprocess_evo2.json
PREPROC_OUTPUT_DIR=/efs/datasets/evo2_preprocessed/sequences
OUTPUT_PREFIX=sequences

# Training config
TRAIN_DATASET_CONFIG=sft/train_evo2.yaml
NUM_NODES=1
DEVICES=1
SEQ_LENGTH=2048
TP_SIZE=1
PP_SIZE=1
CP_SIZE=1
EXPERIMENT_DIR=/efs/projects/PlasmidRL/results/evo2

# Training knobs for memory
MODEL_SIZE=1b
MICRO_BATCH_SIZE=5
GRAD_ACC_BATCHES=12
ACT_CKPT_LAYERS=1
FP8=false
WORKERS=8
CKPT_DIR=/efs/models/nemo2_evo2_1b_8k
CLIP_GRAD=1.0

# Weights & Biases
# Set your WANDB_API_KEY here or export it in your shell before running
WANDB_ENTITY=mcclain
WANDB_PROJECT=evo2-sft
WANDB_TAGS="evo2 plasmidrl sft"
WANDB_GROUP=
WANDB_JOB_TYPE=finetune
WANDB_RUN_NAME=

