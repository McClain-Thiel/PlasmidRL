# W&B Sweep Configuration for PlasmidRL GRPO - Refined Training (500 steps)
# Focus: Refined hyperparameter search based on initial sweep findings
# Duration: 500 steps per trial for more stable evaluation
#
# Key Insights from Initial Sweep:
# - Batch size 16 + 8 generations is best (dominates top 15 runs)
# - Learning rate sweet spot: ~4e-05 to 8e-05
# - High epsilon helps: 0.27-0.30
# - Beta: 0.0008-0.0023 range appears frequently
# - Temperature: 1.07-1.39 all viable
# - Top-p: 0.86-0.93 clustering
#
# Usage:
#   1. Initialize the sweep:
#      wandb sweep sweep_config_refined.yaml
#
#   2. Copy the sweep ID from output (e.g., mcclain/plasmidrl-grpo-sweeps/abc123xyz)
#
#   3. Run agent(s):
#      SWEEP_ID=mcclain/plasmidrl-grpo-sweeps/abc123xyz docker compose up grpo-sweep
#
# Monitor progress at: https://wandb.ai/mcclain/plasmidrl-grpo-sweeps

program: sweeps/run_sweep_agent.py
method: bayes
metric:
  name: reward_components/total_reward/mean
  goal: maximize

parameters:
  # ==================== TRAINING HYPERPARAMETERS (REFINED RANGES) ====================
  
  max_steps:
    value: 500  # Longer runs for stable evaluation
  
  # Learning rate - narrow range around sweet spot
  learning_rate:
    distribution: log_uniform_values
    min: 3.5e-05
    max: 9.0e-05
  
  # Batch size - focus on best performing config
  per_device_train_batch_size:
    values: [16]  # 16 dominated top runs
  
  # Generations - focus on best performing config
  num_generations:
    values: [8]  # 8 generations worked best
  
  # Temperature - narrow range around viable values
  temperature:
    distribution: uniform
    min: 1.05
    max: 1.40
  
  # Top-p - narrow range around cluster
  top_p:
    distribution: uniform
    min: 0.86
    max: 0.93
  
  # Beta - focus on frequently appearing range
  beta:
    distribution: log_uniform_values
    min: 0.0008
    max: 0.0023
  
  # Epsilon - favor high exploration
  epsilon:
    distribution: uniform
    min: 0.27
    max: 0.30
  
  # ==================== REWARD CONFIG (FIXED - BEST FROM INITIAL SWEEP) ====================
  
  reward_punish_mode:
    value: true
  
  reward_length_penalty:
    value: false
  
  reward_min_length:
    value: 1000
  
  reward_max_length:
    value: 30000
  
  reward_promoter_max:
    value: 5
  
  reward_terminator_max:
    value: 2
  
  reward_marker_max:
    value: 2
  
  reward_cds_max:
    value: 5
  
  reward_location_aware:
    value: true
  
  # Reward component weights (all enabled with standard weights)
  reward_ori_weight:
    value: 1.0
  
  reward_promoter_weight:
    value: 1.0
  
  reward_terminator_weight:
    value: 0.5
  
  reward_marker_weight:
    value: 1.0
  
  reward_cds_weight:
    value: 1.0

# Early termination - adjusted for longer runs
early_terminate:
  type: hyperband
  min_iter: 100  # Higher threshold for 500-step runs
  eta: 3
  s: 2

# Optional: Limit total number of runs
# run_cap: 30

