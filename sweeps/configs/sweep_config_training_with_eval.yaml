# W&B Sweep Configuration for PlasmidRL GRPO - Training + Length Reward + Eval
# Focus: Broad training hyperparameter search with length-based rewards and evaluation
# Duration: 500 steps per trial for stable evaluation
# Includes: Evaluation callback to track detailed sequence analysis
#
# Strategy: Test 2 length configurations across full hyperparameter space
#
# Usage:
#   1. Initialize the sweep:
#      wandb sweep sweeps/configs/sweep_config_training_with_eval.yaml
#
#   2. Copy the sweep ID from output (e.g., mcclain/plasmidrl-grpo-sweeps/abc123xyz)
#
#   3. Run agent(s):
#      SWEEP_ID=mcclain/plasmidrl-grpo-sweeps/abc123xyz docker compose up grpo-sweep
#
# Monitor progress at: https://wandb.ai/mcclain/plasmidrl-grpo-sweeps

program: sweeps/run_sweep_agent.py
method: bayes
metric:
  name: reward_components/total_reward/mean
  goal: maximize

parameters:
  # ==================== TRAINING HYPERPARAMETERS (BROAD SEARCH) ====================
  
  max_steps:
    value: 500  # Longer runs for stable evaluation
  
  # Learning rate - broad exploration
  learning_rate:
    distribution: log_uniform_values
    min: 1e-6
    max: 1e-4
  
  # Batch size
  per_device_train_batch_size:
    values: [8, 16, 32]
  
  # Generations per batch
  num_generations:
    values: [4, 8, 16]
  
  # Temperature - sampling randomness
  temperature:
    distribution: uniform
    min: 0.7
    max: 1.4
  
  # Top-p - nucleus sampling
  top_p:
    distribution: uniform
    min: 0.85
    max: 0.95
  
  # Beta - KL penalty coefficient
  beta:
    distribution: log_uniform_values
    min: 1e-4
    max: 1e-2
  
  # Epsilon - PPO-style clipping
  epsilon:
    distribution: uniform
    min: 0.1
    max: 0.3
  
  # ==================== LENGTH REWARD PARAMETERS (2 COMBOS) ====================
  
  reward_length_reward_mode:
    value: true  # Enable length-based rewards
  
  # Test 2 length configurations
  # Combo 1: min=2000, ideal_min=3000, ideal_max=12000, max=15000
  # Combo 2: min=5000, ideal_min=7000, ideal_max=20000, max=30000
  
  reward_min_length:
    values: [2000, 5000]
  
  reward_max_length:
    values: [15000, 30000]
  
  reward_ideal_min_length:
    values: [3000, 7000]
  
  reward_ideal_max_length:
    values: [12000, 20000]
  
  # Bonus multiplier for being in ideal range
  reward_length_reward_bonus:
    distribution: uniform
    min: 0.3
    max: 0.8
  
  # ==================== REWARD CONFIG (FIXED - STANDARD) ====================
  
  reward_punish_mode:
    value: true
  
  reward_promoter_max:
    value: 5
  
  reward_terminator_max:
    value: 2
  
  reward_marker_max:
    value: 2
  
  reward_cds_max:
    value: 5
  
  reward_location_aware:
    value: true
  
  # Reward component weights (all enabled with standard weights)
  reward_ori_weight:
    value: 1.0
  
  reward_promoter_weight:
    value: 1.0
  
  reward_terminator_weight:
    value: 0.5
  
  reward_marker_weight:
    value: 1.0
  
  reward_cds_weight:
    value: 1.0

# Early termination - adjusted for longer runs
early_terminate:
  type: hyperband
  min_iter: 100  # Higher threshold for 500-step runs
  eta: 3
  s: 2

# Optional: Limit total number of runs
# run_cap: 50




