# W&B Sweep Configuration for PlasmidRL GRPO Training
# Focus: Training hyperparameters only (reward config held constant)
#
# Usage:
#   1. Initialize the sweep:
#      wandb sweep sweep_config_training.yaml
#
#   2. Copy the sweep ID from output (e.g., mcclain/plasmidrl-grpo-sweeps/abc123xyz)
#
#   3. Run agent(s):
#      SWEEP_ID=mcclain/plasmidrl-grpo-sweeps/abc123xyz docker compose up grpo-sweep
#
# Monitor progress at: https://wandb.ai/mcclain/plasmidrl-grpo-sweeps

program: sweeps/run_sweep_agent.py
method: bayes
metric:
  name: reward_components/total_reward/mean
  goal: maximize

parameters:
  # ==================== TRAINING HYPERPARAMETERS (SWEPT) ====================
  learning_rate:
    distribution: log_uniform_values
    min: 1e-6
    max: 1e-4
  
  per_device_train_batch_size:
    values: [4, 8, 16]
  
  num_generations:
    values: [2, 4, 8]
  
  temperature:
    distribution: uniform
    min: 0.7
    max: 1.4
  
  top_p:
    distribution: uniform
    min: 0.85
    max: 0.95
  
  # GRPO-specific parameters
  beta:
    distribution: log_uniform_values
    min: 1e-4
    max: 1e-2
  
  epsilon:
    distribution: uniform
    min: 0.1
    max: 0.3
  
  # ==================== REWARD CONFIG (FIXED) ====================
  # All reward parameters held constant for this sweep
  
  reward_punish_mode:
    value: true
  
  reward_length_penalty:
    value: false
  
  reward_min_length:
    value: 1000
  
  reward_max_length:
    value: 30000
  
  reward_promoter_max:
    value: 5
  
  reward_terminator_max:
    value: 2
  
  reward_marker_max:
    value: 2
  
  reward_cds_max:
    value: 5
  
  reward_location_aware:
    value: true
  
  # Reward component weights (all enabled with standard weights)
  reward_ori_weight:
    value: 1.0
  
  reward_promoter_weight:
    value: 1.0
  
  reward_terminator_weight:
    value: 0.5
  
  reward_marker_weight:
    value: 1.0
  
  reward_cds_weight:
    value: 1.0

# Early termination to stop poorly performing runs
early_terminate:
  type: hyperband
  min_iter: 20
  eta: 3
  s: 2

# Optional: Limit total number of runs
# run_cap: 50



