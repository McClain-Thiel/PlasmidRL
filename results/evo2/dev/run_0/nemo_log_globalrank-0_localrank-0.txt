[NeMo I 2025-09-17 11:45:39 nemo_logging:393] Using byte-level tokenization
[NeMo W 2025-09-17 11:45:39 nemo_logging:405] User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Experiments will be logged at results/evo2/dev
[NeMo W 2025-09-17 11:45:40 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/evo2/checkpoints. Training from scratch.
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has data parallel group : [0]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Ranks 0 has data parallel rank: 0
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has context parallel group: [0]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] All context parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Ranks 0 has context parallel rank: 0
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has model parallel group: [0]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] All model parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] All tensor model parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has embedding group: [0]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] All pipeline model parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] All embedding group ranks: [[0]]
[NeMo I 2025-09-17 11:45:40 nemo_logging:393] Rank 0 has embedding rank: 0
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Using byte-level tokenization
[NeMo W 2025-09-17 11:49:00 nemo_logging:405] User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Experiments will be logged at results/evo2/dev
[NeMo W 2025-09-17 11:49:00 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/evo2/checkpoints. Training from scratch.
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has data parallel group : [0]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Ranks 0 has data parallel rank: 0
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has context parallel group: [0]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] All context parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Ranks 0 has context parallel rank: 0
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has model parallel group: [0]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] All model parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] All tensor model parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has embedding group: [0]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] All pipeline model parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] All embedding group ranks: [[0]]
[NeMo I 2025-09-17 11:49:00 nemo_logging:393] Rank 0 has embedding rank: 0
[NeMo I 2025-09-17 11:49:03 utils:661] Building Evo2Dataset splits with sizes=[500000, 10000020, 1] and config=GPTDatasetConfig(random_seed=1234, sequence_length=8192, blend=None, blend_per_split=[(['/mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_train'], [1.0]), (['/mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_val'], [1.0]), (['/mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_test'], [1.0])], split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.bytelevel_tokenizers.ByteLevelTokenizer object at 0x7a3f6acc2750>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)
[NeMo I 2025-09-17 11:49:03 utils:661] Load the _IndexReader from /mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_train.idx
[NeMo I 2025-09-17 11:49:03 utils:661] 	Extract the sequence lengths
[NeMo I 2025-09-17 11:49:03 utils:661] 	Extract the sequence pointers
[NeMo I 2025-09-17 11:49:03 utils:661] 	Extract the document indices
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of sequences: 58078
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of documents: 58078
[NeMo I 2025-09-17 11:49:03 utils:661] Build and save the Evo2Dataset train indices
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of samples: 715619
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of epochs: 1
[NeMo I 2025-09-17 11:49:03 utils:661] Load the _IndexReader from /mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_val.idx
[NeMo I 2025-09-17 11:49:03 utils:661] 	Extract the sequence lengths
[NeMo I 2025-09-17 11:49:03 utils:661] 	Extract the sequence pointers
[NeMo I 2025-09-17 11:49:03 utils:661] 	Extract the document indices
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of sequences: 7311
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of documents: 7311
[NeMo I 2025-09-17 11:49:03 utils:661] Build and save the Evo2Dataset valid indices
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of samples: 10035048
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of epochs: 113
[NeMo I 2025-09-17 11:49:03 utils:661] Load the _IndexReader from /mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_test.idx
[NeMo I 2025-09-17 11:49:03 utils:661] 	Extract the sequence lengths
[NeMo I 2025-09-17 11:49:03 utils:661] 	Extract the sequence pointers
[NeMo I 2025-09-17 11:49:03 utils:661] 	Extract the document indices
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of sequences: 7167
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of documents: 7167
[NeMo I 2025-09-17 11:49:03 utils:661] Build and save the Evo2Dataset test indices
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of samples: 86412
[NeMo I 2025-09-17 11:49:03 utils:661] > total number of epochs: 1
[NeMo I 2025-09-17 11:49:03 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.
[NeMo W 2025-09-17 11:49:04 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:49:04 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:49:04 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:49:04 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:49:04 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:49:04 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:49:04 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:49:04 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:49:04 random:222] CPU RNG state changed within GPU RNG context
[NeMo I 2025-09-17 11:49:04 nemo_logging:393] Copying Trainer's 'max_steps' (500000) to LR scheduler's 'max_steps'.
[NeMo I 2025-09-17 11:49:04 num_microbatches_calculator:228] setting number of microbatches to constant 1
[NeMo I 2025-09-17 11:49:04 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 6481649408
[NeMo I 2025-09-17 11:49:04 utils:661] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=True, fp8_param_gather=False, reuse_grad_buf_for_mxfp8_param_ag=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False, nccl_ub=False, fsdp_double_buffer=False)
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Using byte-level tokenization
[NeMo W 2025-09-17 11:52:16 nemo_logging:405] User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Experiments will be logged at results/evo2/dev
[NeMo W 2025-09-17 11:52:16 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/evo2/checkpoints. Training from scratch.
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has data parallel group : [0]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Ranks 0 has data parallel rank: 0
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has context parallel group: [0]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] All context parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Ranks 0 has context parallel rank: 0
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has model parallel group: [0]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] All model parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] All tensor model parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has embedding group: [0]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] All pipeline model parallel group ranks: [[0]]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] All embedding group ranks: [[0]]
[NeMo I 2025-09-17 11:52:16 nemo_logging:393] Rank 0 has embedding rank: 0
[NeMo I 2025-09-17 11:52:19 utils:661] Building Evo2Dataset splits with sizes=[8000000, 160000320, 16] and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=None, blend_per_split=[(['/mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_train'], [1.0]), (['/mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_val'], [1.0]), (['/mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_test'], [1.0])], split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.bytelevel_tokenizers.ByteLevelTokenizer object at 0x7ae5181e9f40>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)
[NeMo I 2025-09-17 11:52:19 utils:661] Load the _IndexReader from /mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_train.idx
[NeMo I 2025-09-17 11:52:19 utils:661] 	Extract the sequence lengths
[NeMo I 2025-09-17 11:52:19 utils:661] 	Extract the sequence pointers
[NeMo I 2025-09-17 11:52:19 utils:661] 	Extract the document indices
[NeMo I 2025-09-17 11:52:19 utils:661] > total number of sequences: 58078
[NeMo I 2025-09-17 11:52:19 utils:661] > total number of documents: 58078
[NeMo I 2025-09-17 11:52:19 utils:661] Build and save the Evo2Dataset train indices
[NeMo I 2025-09-17 11:52:19 utils:661] > total number of samples: 8587436
[NeMo I 2025-09-17 11:52:19 utils:661] > total number of epochs: 3
[NeMo I 2025-09-17 11:52:19 utils:661] Load the _IndexReader from /mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_val.idx
[NeMo I 2025-09-17 11:52:19 utils:661] 	Extract the sequence lengths
[NeMo I 2025-09-17 11:52:19 utils:661] 	Extract the sequence pointers
[NeMo I 2025-09-17 11:52:19 utils:661] 	Extract the document indices
[NeMo I 2025-09-17 11:52:19 utils:661] > total number of sequences: 7311
[NeMo I 2025-09-17 11:52:19 utils:661] > total number of documents: 7311
[NeMo I 2025-09-17 11:52:19 utils:661] Build and save the Evo2Dataset valid indices
[NeMo I 2025-09-17 11:52:31 utils:661] > total number of samples: 160205548
[NeMo I 2025-09-17 11:52:31 utils:661] > total number of epochs: 451
[NeMo I 2025-09-17 11:52:31 utils:661] Load the _IndexReader from /mcclain/datasets/evo2_preprocessed/sequences/sequences_byte-level_test.idx
[NeMo I 2025-09-17 11:52:31 utils:661] 	Extract the sequence lengths
[NeMo I 2025-09-17 11:52:31 utils:661] 	Extract the sequence pointers
[NeMo I 2025-09-17 11:52:31 utils:661] 	Extract the document indices
[NeMo I 2025-09-17 11:52:31 utils:661] > total number of sequences: 7167
[NeMo I 2025-09-17 11:52:31 utils:661] > total number of documents: 7167
[NeMo I 2025-09-17 11:52:31 utils:661] Build and save the Evo2Dataset test indices
[NeMo I 2025-09-17 11:52:31 utils:661] > total number of samples: 345648
[NeMo I 2025-09-17 11:52:31 utils:661] > total number of epochs: 1
[NeMo I 2025-09-17 11:52:31 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.
[NeMo W 2025-09-17 11:52:31 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:52:31 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:52:31 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:52:31 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:52:31 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:52:31 random:222] CPU RNG state changed within GPU RNG context
[NeMo W 2025-09-17 11:52:31 random:222] CPU RNG state changed within GPU RNG context
[NeMo I 2025-09-17 11:52:31 nemo_logging:393] Copying Trainer's 'max_steps' (500000) to LR scheduler's 'max_steps'.
[NeMo I 2025-09-17 11:52:31 num_microbatches_calculator:228] setting number of microbatches to constant 16
[NeMo I 2025-09-17 11:52:31 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1108204800
[NeMo I 2025-09-17 11:52:31 utils:661] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=True, fp8_param_gather=False, reuse_grad_buf_for_mxfp8_param_ag=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False, nccl_ub=False, fsdp_double_buffer=False)
[NeMo I 2025-09-17 11:52:31 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1
    Params for bucket 1 (1108204800 elements, 1108204800 padded size):
    	module.decoder.layers.24.self_attention.linear_proj.weight
    	module.decoder.layers.21.mixer.mixer.short_conv.short_conv_weight
    	module.decoder.layers.19.mixer.dense.bias
    	module.decoder.layers.18.mlp.linear_fc2.weight
    	module.decoder.layers.14.mixer.mixer.short_conv.short_conv_weight
    	module.decoder.layers.11.mlp.linear_fc2.weight
    	module.decoder.layers.6.mixer.dense.weight
    	module.decoder.layers.5.mixer.dense_projection.weight
    	module.decoder.layers.0.mixer.dense.bias
    	module.decoder.layers.0.mixer.dense_projection.weight
    	module.decoder.layers.22.mlp.linear_fc2.weight
    	module.decoder.layers.19.mlp.linear_fc1.weight
    	module.decoder.layers.12.mlp.linear_fc1.weight
    	module.decoder.layers.8.mixer.mixer.conv_bias
    	module.decoder.layers.5.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.23.mixer.dense.weight
    	module.decoder.layers.18.mixer.dense.weight
    	module.decoder.layers.16.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.15.mixer.mixer.conv_bias
    	module.decoder.layers.11.mixer.dense.weight
    	module.decoder.layers.9.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_proj.bias
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.0.mixer.mixer.short_conv.short_conv_weight
    	module.decoder.layers.24.mlp.linear_fc1.weight
    	module.decoder.layers.22.mixer.dense.bias
    	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.mixer.dense.weight
    	module.decoder.layers.1.mixer.dense.bias
    	module.decoder.layers.20.mixer.mixer.filter.R
    	module.decoder.layers.18.mixer.dense.bias
    	module.decoder.layers.17.mlp.linear_fc2.weight
    	module.decoder.layers.13.mixer.mixer.filter.R
    	module.decoder.layers.10.mlp.linear_fc2.weight
    	module.decoder.layers.9.mixer.mixer.conv_bias
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.mixer.dense.weight
    	module.decoder.layers.22.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.19.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.12.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.7.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.23.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.19.mlp.linear_fc2.weight
    	module.decoder.layers.15.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.12.mlp.linear_fc2.weight
    	module.decoder.layers.8.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.5.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.2.mixer.mixer.filter.p
    	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.20.mixer.dense.weight
    	module.decoder.layers.16.mixer.mixer.filter.gamma
    	module.decoder.layers.13.mixer.dense.weight
    	module.decoder.layers.9.mixer.mixer.filter.gamma
    	module.decoder.layers.5.mixer.dense.weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.24.mlp.linear_fc2.weight
    	module.decoder.layers.22.mixer.mixer.filter.h
    	module.decoder.layers.16.mlp.linear_fc2.weight
    	module.decoder.layers.15.mixer.mixer.filter.h
    	module.decoder.layers.9.mlp.linear_fc2.weight
    	module.decoder.layers.6.mixer.mixer.conv_bias
    	module.decoder.layers.2.mixer.mixer.filter.gamma
    	module.decoder.layers.18.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.15.mixer.dense.bias
    	module.decoder.layers.15.mixer.dense.weight
    	module.decoder.layers.11.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.8.mixer.dense.weight
    	module.decoder.layers.6.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.1.mixer.dense_projection.weight
    	module.decoder.layers.21.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.16.mixer.mixer.filter.p
    	module.decoder.layers.14.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.9.mixer.mixer.filter.p
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.embedding.word_embeddings.weight
    	module.decoder.layers.23.mixer.mixer.filter.gamma
    	module.decoder.layers.20.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.15.mixer.dense_projection.weight
    	module.decoder.layers.13.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.8.mixer.dense_projection.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mixer.mixer.filter.R
    	module.decoder.layers.4.mixer.mixer.short_conv.short_conv_weight
    	module.decoder.layers.1.mixer.mixer.conv_bias
    	module.decoder.layers.23.mlp.linear_fc2.weight
    	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.16.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.9.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mixer.dense_projection.weight
    	module.decoder.layers.22.mixer.dense.weight
    	module.decoder.layers.22.mixer.dense_projection.weight
    	module.decoder.layers.21.mixer.dense.bias
    	module.decoder.layers.16.mlp.linear_fc1.weight
    	module.decoder.layers.14.mixer.dense.bias
    	module.decoder.layers.9.mlp.linear_fc1.weight
    	module.decoder.layers.8.mixer.mixer.filter.h
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.23.mixer.mixer.filter.p
    	module.decoder.layers.18.mixer.mixer.short_conv.short_conv_weight
    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.11.mixer.mixer.short_conv.short_conv_weight
    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.mixer.dense_projection.weight
    	module.decoder.layers.4.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.2.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.22.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.19.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.12.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.7.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.6.mixer.dense.bias
    	module.decoder.layers.0.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.0.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.23.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.20.mixer.mixer.filter.gamma
    	module.decoder.layers.19.mixer.mixer.conv_bias
    	module.decoder.layers.13.mixer.mixer.filter.gamma
    	module.decoder.layers.12.mixer.dense.bias
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.23.mlp.linear_fc1.weight
    	module.decoder.layers.20.mlp.linear_fc2.weight
    	module.decoder.layers.19.mixer.mixer.filter.h
    	module.decoder.layers.16.mixer.dense_projection.weight
    	module.decoder.layers.13.mlp.linear_fc2.weight
    	module.decoder.layers.12.mixer.mixer.filter.h
    	module.decoder.layers.9.mixer.dense_projection.weight
    	module.decoder.layers.6.mixer.mixer.filter.p
    	module.decoder.layers.5.mixer.mixer.filter.h
    	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.21.mlp.linear_fc1.weight
    	module.decoder.layers.19.mixer.dense.weight
    	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.14.mlp.linear_fc1.weight
    	module.decoder.layers.12.mixer.dense.weight
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.6.mixer.mixer.filter.gamma
    	module.decoder.layers.6.mixer.dense_projection.weight
    	module.decoder.layers.20.mixer.mixer.filter.p
    	module.decoder.layers.18.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.16.mixer.mixer.conv_bias
    	module.decoder.layers.13.mixer.mixer.filter.p
    	module.decoder.layers.11.mixer.dense.bias
    	module.decoder.layers.11.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.23.mixer.mixer.conv_bias
    	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.19.mixer.dense_projection.weight
    	module.decoder.layers.16.mixer.dense.bias
    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.12.mixer.dense_projection.weight
    	module.decoder.layers.9.mixer.dense.bias
    	module.decoder.layers.7.mixer.mixer.short_conv.short_conv_weight
    	module.decoder.layers.4.mixer.dense.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.23.mixer.dense_projection.weight
    	module.decoder.layers.20.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.17.self_attention.linear_qkv.weight
    	module.decoder.layers.13.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.10.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mixer.mixer.conv_bias
    	module.decoder.layers.0.mixer.dense.weight
    	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.20.mlp.linear_fc1.weight
    	module.decoder.layers.13.mlp.linear_fc1.weight
    	module.decoder.layers.12.mixer.mixer.conv_bias
    	module.decoder.layers.6.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.1.mixer.dense.weight
    	module.decoder.layers.21.mlp.linear_fc2.weight
    	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.14.mlp.linear_fc2.weight
    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.2.mixer.mixer.conv_bias
    	module.decoder.layers.23.mixer.dense.bias
    	module.decoder.layers.18.mixer.dense_projection.weight
    	module.decoder.layers.15.mlp.linear_fc1.weight
    	module.decoder.layers.11.mixer.dense_projection.weight
    	module.decoder.layers.8.mlp.linear_fc1.weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.1.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.2.mixer.dense.bias
    	module.decoder.layers.24.self_attention.linear_qkv.weight
    	module.decoder.layers.24.self_attention.linear_proj.bias
    	module.decoder.layers.21.mixer.dense_projection.weight
    	module.decoder.layers.14.mixer.dense_projection.weight
    	module.decoder.layers.7.mixer.dense.bias
    	module.decoder.layers.4.mixer.hyena_proj_conv.short_conv_weight
    	module.decoder.layers.2.mixer.mixer.filter.R
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.20.mixer.dense_projection.weight
    	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.13.mixer.dense_projection.weight
    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.final_norm.weight
    	module.decoder.layers.21.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.18.mlp.linear_fc1.weight
    	module.decoder.layers.16.mixer.mixer.filter.R
    	module.decoder.layers.14.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.11.mlp.linear_fc1.weight
    	module.decoder.layers.9.mixer.mixer.filter.R
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.17.self_attention.linear_proj.weight
    	module.decoder.layers.22.mlp.linear_fc1.weight
    	module.decoder.layers.20.mixer.mixer.conv_bias
    	module.decoder.layers.15.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.13.mixer.mixer.conv_bias
    	module.decoder.layers.10.self_attention.linear_proj.weight
    	module.decoder.layers.8.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.mixer.dense.bias
    	module.decoder.layers.1.mixer.mixer.filter.h
    	module.decoder.layers.17.self_attention.linear_proj.bias
    	module.decoder.layers.20.mixer.dense.bias
    	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.15.mlp.linear_fc2.weight
    	module.decoder.layers.13.mixer.dense.bias
    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.10.self_attention.linear_proj.bias
    	module.decoder.layers.8.mlp.linear_fc2.weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.22.mixer.mixer.conv_bias
    	module.decoder.layers.21.mixer.dense.weight
    	module.decoder.layers.16.mixer.dense.weight
    	module.decoder.layers.14.mixer.dense.weight
    	module.decoder.layers.9.mixer.dense.weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mixer.dense_projection.layer_norm_weight
    	module.decoder.layers.23.mixer.mixer.filter.R
    	module.decoder.layers.17.mlp.linear_fc1.weight
    	module.decoder.layers.10.mlp.linear_fc1.weight
    	module.decoder.layers.8.mixer.dense.bias
    	module.decoder.layers.4.mixer.dense.bias
    	module.decoder.layers.4.mixer.dense_projection.weight
    	module.decoder.layers.2.mixer.hyena_proj_conv.short_conv_weight
[NeMo I 2025-09-17 11:52:31 utils:661] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0003, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp8_recipe='delayed', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
[NeMo I 2025-09-17 11:52:32 nemo_logging:393] Doing selective restore from RestoreConfig(path='/mcclain/models/nemo2_evo2_1b_8k', load_model_state=True, load_optim_state=False, load_artifacts=True)
[NeMo I 2025-09-17 11:52:32 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7ae4d4eb6e10> dist-ckpt load strategy.
[NeMo I 2025-09-17 11:52:39 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1758109952.107s : Time spent in load_checkpoint: 7.669s
[NeMo I 2025-09-17 11:52:39 nemo_logging:393] Restoring model weights from RestoreConfig(path='/mcclain/models/nemo2_evo2_1b_8k', load_model_state=True, load_optim_state=False, load_artifacts=True)
[NeMo I 2025-09-17 11:52:39 nemo_logging:393] Finished restoring from RestoreConfig(path='/mcclain/models/nemo2_evo2_1b_8k', load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.
